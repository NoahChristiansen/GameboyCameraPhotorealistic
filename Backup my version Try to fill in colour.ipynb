{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In 1998 Nintendo released the Gameboy Camera. With this camera it was possible to take images in a resolution of 256x224 pixels (or 0.05734 megapixels). The screen resized your image to 190x144 pixels, and shows it in 4 shades of gray/green. Despite these limitations images you took are recognisable for us humans. In this post I show my adventures in improving the camera images using Deep Neural Networks!\n",
    "\n",
    "\n",
    "[expand title=\"view helper functions\"]something here[/expand]\n",
    "\n",
    "Recently several applications of convolutional neural networks have been discovered. Examples are super-resolution (upscaling an image without loss), coloring (from grayscale to RGB), and <a href=\"https://arxiv.org/pdf/1504.06993.pdf\" target=\"_blank\">removing (JPEG) compression artifacts</a>. Another example is turning <a href=\"\n",
    "https://arxiv.org/pdf/1606.03073v1.pdf\" target=\"_blank\">sketches into photorealistic face images</a>, discovered by Yağmur Güçlütürk, Umut Güçlü, Rob van Lier, and Marcel van Gerven. This last example inspired me to take gameboy camera images of faces and turn them into photorealistic images.\n",
    "\n",
    "XXX IMAGE HERE OF THE GAMEBOY CAMERA XXX\n",
    "\n",
    "Back in 1998, the gameboy camera got the world record as \"smallest digital camera\" in the Guinness book of records. An accessory you could buy was the small printer you could use to print your images. When I was 10 years old we has one of these cameras at home, and used it a lot. Although we did not have the printer, taking pictures, editing them, and playing minigames was a lot of fun. Unfortunately I could not find my old camera (no colored young Roland pictures unfortunately), but I did buy a new one so I could test my application.\n",
    "\n",
    "In the end the result turned out very good. The generated images are really great. Although we trained on a small part of the face even pictures of whole heads seem to turn out nice.\n",
    "\n",
    "XXX IMAGE HERE OF THE RESULTS XXX\n",
    "\n",
    "In this blogpost I will guide you through my progress of this project. Some boring parts are hidden, but can be expanded for the full story. With the code you should be able to replicate the results. <a href=\"https://github.com/rmeertens/convolutional_color_gameboy_camera\" target=\"_blank\">A Git repository can be found here.</a> \n",
    "\n",
    "### Training data\n",
    "\n",
    "Unfortunately, there is no training-data set with gameboy-camera images of faces together with the real picture of the person. To create a dataset I made a function that takes an image as input and creates an image with 4 shades of black. The shade is based on the mean and standard deviation of the image, to make sure that we always use 4 colors. If you look at original gameboy camera images you can see that they create gradients by alternating pixels to give the illusion of more colors. To immitate this I simply added noise on top of my original images. Note that if you want to experiment you can change the apply_effect_on_folder function to create images from sketches instead of gameboy camera images. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/usr/local/lib/python3.4/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "libcudart.so.7.5: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e91c0110a60d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlibs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvgg16\u001b[0m \u001b[0;31m# Download here! https://github.com/pkmital/CADL/tree/master/session-4/libs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: libcudart.so.7.5: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipyd\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import cv2 \n",
    "from sklearn.decomposition import PCA\n",
    "from libs import vgg16 # Download here! https://github.com/pkmital/CADL/tree/master/session-4/libs\n",
    "from libs import gif\n",
    "IMAGE_PATH = \"/home/roland/img_align_celeba_png\" # DOWNLOAD HERE! http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\n",
    "PICTURE_DATASET = os.listdir(IMAGE_PATH)\n",
    "\n",
    "PREPROCESSED_IMAGE_PATH = \"/home/roland/img_align_celeba_effect\"\n",
    "PROCESSED_PICTURE_DATASET = os.listdir(PREPROCESSED_IMAGE_PATH)\n",
    "\n",
    "IMAGE_WIDTH = 96\n",
    "IMAGE_HEIGHT = 96\n",
    "COLOR_CHANNEL_COUNT = 3\n",
    "NORMALISE_INPUT = False\n",
    "\n",
    "def load_random_picture_from_list(image_names,path):\n",
    "    index_image = random.randint(0,len(image_names)-1)\n",
    "    name_picture = image_names[index_image]\n",
    "    path_file = os.path.join(path,name_picture)\n",
    "    image = plt.imread(path_file)\n",
    "    return image\n",
    "    \n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "\n",
    "def add_sketch_effect(image):\n",
    "    img_gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "    img_gray_inv = 255 - img_gray\n",
    "    img_blur = cv2.GaussianBlur(img_gray_inv, ksize=(5, 5),sigmaX=0, sigmaY=0)\n",
    "\n",
    "    img_blend = dodgeV2(img_gray, img_blur)\n",
    "    ret,img_blend = cv2.threshold(img_blend,240,255,cv2.THRESH_TRUNC)\n",
    "    return img_blend\n",
    "\n",
    "def add_gameboy_camera_effect(image):\n",
    "    img_gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "    mean = np.mean(img_gray)\n",
    "    stdev = np.std(img_gray)\n",
    "    random_noise = np.random.random_sample(size=img_gray.shape)/10\n",
    "    img_gray += random_noise\n",
    "    lowest = img_gray < (mean-stdev)\n",
    "    second = (img_gray < (mean)) \n",
    "    third = (img_gray < (mean+stdev))\n",
    "    highest = (img_gray >= 0.0)-third\n",
    "    pallet = np.zeros(img_gray.shape,dtype=np.float32)\n",
    "    pallet[highest]=1.0\n",
    "    pallet[third]=0.66\n",
    "    pallet[second]=0.33\n",
    "    pallet[lowest]=0.0\n",
    "    return pallet\n",
    "\n",
    "\n",
    "def dodgeV2(image, mask):\n",
    "    return cv2.divide(image, 255-mask, scale=256)\n",
    "\n",
    "\n",
    "def burnV2(image, mask):\n",
    "    return 255 - cv2.divide(255-image, 255-mask, scale=256)\n",
    "\n",
    "\n",
    "def resize_image_by_cropping(image,width,height):\n",
    "    \"\"\"Resizes image by cropping the relevant part out of the image\"\"\"\n",
    "    original_height = len(image)\n",
    "    original_width = len(image[0])\n",
    "    start_h = (original_height - height)//2\n",
    "    start_w = (original_width - width)//2\n",
    "    return image[start_h:start_h+height,start_w:start_w+width]\n",
    "\n",
    "\n",
    "def apply_effect_on_folder(name_input_folder,name_output_folder):\n",
    "    picture_names = os.listdir(name_input_folder)\n",
    "    i = 0\n",
    "    for name_picture in picture_names:\n",
    "        i+=1\n",
    "        if i % 250==1:\n",
    "            print(i)\n",
    "            print(len(picture_names))\n",
    "        path_file = os.path.join(IMAGE_PATH,name_picture)\n",
    "        image = plt.imread(path_file)\n",
    "        image = resize_image_by_cropping(image)\n",
    "        effect = add_gameboy_camera_effect(image)\n",
    "        write_path_original = os.path.join(name_output_folder,name_picture+\".orig\")\n",
    "        write_path_effect = os.path.join(name_output_folder,name_picture+\".effect\")\n",
    "        \n",
    "        np.save(write_path_original,image)\n",
    "        np.save(write_path_effect,effect)\n",
    "        \n",
    "def load_names_images():\n",
    "    names_images = [a[:6] for a in PROCESSED_PICTURE_DATASET]\n",
    "    names_images = list(set(names_images))\n",
    "    orig = [a+\".png.orig.npy\" for a in names_images]\n",
    "    effect = [a+\".png.effect.npy\" for a in names_images]\n",
    "    return list(zip(orig,effect))\n",
    "\n",
    "def normalise_image(image,mean,stdev):\n",
    "    normalised = (image-mean)/stdev\n",
    "    return normalised\n",
    "\n",
    "def normalise_numpy_images(images,mean,stdev):\n",
    "    return np.array([normalise_image(image,mean,stdev) for image in images])\n",
    "\n",
    "def denormalise_image(image,mean,stdev):\n",
    "    return image*mean+stdev\n",
    "\n",
    "\n",
    "class PreprocessedImageLoader:\n",
    "    def get_random_images_from_set(self,count,names_images):\n",
    "        Xs = []\n",
    "        Ys = []\n",
    "        for _ in range(count):           \n",
    "            index = random.randint(0,len(names_images)-1)\n",
    "            name_orig = os.path.join(self.path,names_images[index][0])\n",
    "            name_effect = os.path.join(self.path,names_images[index][1])\n",
    "            Xs.append( np.load(name_effect))\n",
    "            Ys.append( np.load(name_orig))\n",
    "        return np.array(Xs),np.array(Ys)\n",
    "    \n",
    "    def get_train_images(self,count):\n",
    "        return self.get_random_images_from_set(count,self.trainimage_names)\n",
    "    \n",
    "    def get_test_images(self,count):\n",
    "        return self.get_random_images_from_set(count,self.testimage_names)\n",
    "    \n",
    "    def __init__(self,path,image_names,trainsplit_ratio=0.8):\n",
    "        assert trainsplit_ratio > 0.0\n",
    "        assert trainsplit_ratio < 1.0\n",
    "        self.path = path\n",
    "        self.trainimage_names = names_images[:int(trainsplit_ratio*len(image_names))]\n",
    "        self.testimage_names = names_images[int(trainsplit_ratio*len(image_names)):]\n",
    "\n",
    "#apply_effect_on_folder(IMAGE_PATH,PREPROCESSED_IMAGE_PATH)\n",
    "names_images = load_names_images()\n",
    "imageloader = PreprocessedImageLoader(PREPROCESSED_IMAGE_PATH,names_images)\n",
    "\n",
    "source_x, test_y = imageloader.get_test_images(10)\n",
    "fig = plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.imshow(source_x[0],cmap='gray')\n",
    "plt.subplot(122)\n",
    "plt.imshow(test_y[0])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "libcudart.so.7.5: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-72fbbcfe2587>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: libcudart.so.7.5: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "import cv2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As you can see the random noise on top of the image creates the \"gradients\" you see in the gameboy camera images that give the illusion of more than 4 colors. Note that a downside of the crop function I programmed is that the background of the images is not really visible (even parts of the chin are hidden). \n",
    "\n",
    "### Data preprocessing\n",
    "The preprocessing step of the project is normalising the input images. Hidden is the code that loads 30.000 training images and calculates the mean and standard deviation of the gameboy images and the original images. Just because it looks cool, this is the mean of both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "name_save_mean_std = \"mean_std_sketches.npy\"\n",
    "name_save_color_mean_std = \"mean_std_color.npy\"\n",
    "if os.path.isfile(name_save_mean_std):\n",
    "    loaded_images = np.load(name_save_mean_std)\n",
    "    mean_sketch = loaded_images[0]\n",
    "    stdeviation_sketch = loaded_images[1]\n",
    "    loaded_images = np.load(name_save_color_mean_std)\n",
    "    mean_color = loaded_images[0]\n",
    "    stdeviation_color = loaded_images[1]\n",
    "else:\n",
    "    TrainInput, TrainOutput = imageloader.get_train_images(30000) \n",
    "    sketches = np.array(TrainInput)\n",
    "    color_images = np.array(TrainOutput)\n",
    "    mean_sketch = np.mean(sketches,axis=0)\n",
    "    stdeviation_sketch = np.std(sketches,axis=0)\n",
    "    mean_color = np.mean(color_images,axis=0)\n",
    "    stdeviation_color = np.mean(color_images,axis=0)\n",
    "    \n",
    "    to_save = np.array([mean_sketch,stdeviation_sketch])\n",
    "    \n",
    "    np.save(name_save_mean_std,to_save)\n",
    "    to_save = np.array([mean_color,stdeviation_color])\n",
    "    np.save(name_save_color_mean_std,to_save)\n",
    "\n",
    "def normalise_image(image,mean,stdev):\n",
    "    normalised = (image-mean)/stdev\n",
    "    return normalised\n",
    "\n",
    "def normalise_numpy_images(images,mean,stdev):\n",
    "    return np.array([normalise_image(image,mean,stdev) for image in images])\n",
    "def denormalise_image(image,mean,stdev):\n",
    "    return image*mean+stdev\n",
    "\n",
    "if NORMALISE_INPUT:\n",
    "    test_x = normalise_numpy_images(source_x,mean_sketch,stdeviation_sketch)\n",
    "else:\n",
    "    test_x = source_x\n",
    "test_x = np.expand_dims(test_x,3)\n",
    "progress_images = []\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.imshow(mean_color)\n",
    "plt.subplot(122)\n",
    "plt.imshow(mean_sketch,cmap='gray')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Helper functions\n",
    "To ease the programming I created several helper functions for operations in my graph. These are the functions I used. \n",
    "Note that there are two types of deconvolution layers: the conv2d_transpose and the resize type. \n",
    "In the beginning I only programmed the first one, but this gave me problems with strange patterns on top of my images. \n",
    "\n",
    "XXX INSERT IMAGES XXX\n",
    "\n",
    "Thanks to this post http://distill.pub/2016/deconv-checkerboard/ I found out about the alternative deconvolution layer and implemented that one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def conv_layer(input_image,ksize,in_channels,out_channels,stride,scope_name,activation_function=tf.nn.relu):\n",
    "    with tf.variable_scope(scope_name):\n",
    "        filter = tf.Variable(tf.random_normal([ksize,ksize,in_channels,out_channels],stddev=0.03))\n",
    "        output = tf.nn.conv2d(input_image,filter, strides=[1, stride, stride, 1], padding='SAME')\n",
    "        if activation_function:\n",
    "            output = activation_function(output)\n",
    "        return output, filter\n",
    "    \n",
    "def residual_layer(input_image,ksize,in_channels,out_channels,stride,scope_name):\n",
    "    with tf.variable_scope(scope_name):\n",
    "        output,out_weights = conv_layer(input_image,ksize,in_channels,out_channels,stride,scope_name+\"conv1\")\n",
    "        output,out_weights = conv_layer(output,ksize,out_channels,out_channels,stride,scope_name+\"conv2\")\n",
    "        cool_stuff = tf.add(output,tf.identity(input_image))\n",
    "        return cool_stuff,out_weights\n",
    "    \n",
    "def transpose_deconvolution_layer(input_tensor,used_weights,new_shape,stride,scope_name):\n",
    "    with tf.variable_scope(scope_name):\n",
    "        output = tf.nn.conv2d_transpose(input_tensor, used_weights, output_shape=new_shape,strides=[1,stride,stride,1], padding='SAME')\n",
    "        output = tf.nn.relu(output)\n",
    "        return output\n",
    "    \n",
    "def resize_deconvolution_layer(input_tensor,used_weights,new_shape,stride,scope_name):\n",
    "    with tf.variable_scope(scope_name):\n",
    "        output = tf.image.resize_images(input_tensor,(new_shape[1],new_shape[2]))#tf.nn.conv2d_transpose(input_tensor, used_weights, output_shape=new_shape,strides=[1,stride,stride,1], padding='SAME')\n",
    "        output, unused_weights = conv_layer(output,3,new_shape[3]*2,new_shape[3],1,scope_name+\"_awesome_deconv\")\n",
    "        return output\n",
    "\n",
    "def deconvolution_layer(input_tensor,used_weights,new_shape,stride,scope_name):\n",
    "    return resize_deconvolution_layer(input_tensor,used_weights,new_shape,stride,scope_name)\n",
    "\n",
    "    \n",
    "def output_between_zero_and_one(output):\n",
    "    output +=1 \n",
    "    return output/2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Loss functions\n",
    "\n",
    "In the sketch-to-photorealistic-image paper the authors use three different loss functions: pixel-loss, style-loss, and a smoothing-loss. I use the same functions for my project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_style_layer_vgg16(image):\n",
    "    net = vgg16.get_vgg_model()\n",
    "    style_layer = 'conv2_2/conv2_2:0'\n",
    "    feature_transformed_image = tf.import_graph_def(\n",
    "        net['graph_def'],\n",
    "        name='vgg',\n",
    "        input_map={'images:0': image},return_elements=[style_layer])\n",
    "    feature_transformed_image = (feature_transformed_image[0])\n",
    "    return feature_transformed_image\n",
    "\n",
    "def get_style_loss(target,prediction):\n",
    "    feature_transformed_target = get_style_layer_vgg16(target)    \n",
    "    feature_transformed_prediction = get_style_layer_vgg16(prediction)\n",
    "    feature_count = tf.shape(feature_transformed_target)[3]\n",
    "    style_loss = tf.reduce_sum(tf.square(feature_transformed_target-feature_transformed_prediction))\n",
    "    style_loss = style_loss/tf.cast(feature_count, tf.float32)\n",
    "    return style_loss\n",
    "\n",
    "def get_smooth_loss(image):\n",
    "    batch_count = tf.shape(image)[0]\n",
    "    image_height = tf.shape(image)[1]\n",
    "    image_width = tf.shape(image)[2]    \n",
    "\n",
    "    horizontal_normal = tf.slice(image, [0, 0, 0,0], [batch_count, image_height, image_width-1,3])\n",
    "    horizontal_one_right = tf.slice(image, [0, 0, 1,0], [batch_count, image_height, image_width-1,3])\n",
    "    vertical_normal = tf.slice(image, [0, 0, 0,0], [batch_count, image_height-1, image_width,3])\n",
    "    vertical_one_right = tf.slice(image, [0, 1, 0,0], [batch_count, image_height-1, image_width,3])\n",
    "    smooth_loss = tf.nn.l2_loss(horizontal_normal-horizontal_one_right)+tf.nn.l2_loss(vertical_normal-vertical_one_right)\n",
    "    return smooth_loss\n",
    "\n",
    "def get_pixel_loss(target,prediction):\n",
    "    pixel_difference = target - prediction\n",
    "    pixel_loss = tf.nn.l2_loss(pixel_difference)\n",
    "    return pixel_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### The network\n",
    "The network consists of many convolutional layers for scaling the image down, adding/removing information, and scaling it back to the size we want it to be. The network is inspired by the paper \"Convolutional Sketch Inversion\", but there are some minor difference. One thing I ignored is the batch normalisation layer. Although it is easy to add my network, this network already trained fast enough. Another thing I did was use only two residual layers, this is mostly because of my lack of computing power. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_placeholder = tf.placeholder(tf.float32, [None, IMAGE_HEIGHT,IMAGE_WIDTH,1])\n",
    "output_placeholder = tf.placeholder(tf.float32,[None,IMAGE_HEIGHT,IMAGE_WIDTH,COLOR_CHANNEL_COUNT])\n",
    "computed_batch_size = tf.shape(input_placeholder)[0]\n",
    "\n",
    "conv1, conv1_weights = conv_layer(input_placeholder,9,1,32,1,\"conv1\")\n",
    "conv2, conv2_weights = conv_layer(conv1,3,32,64,2,\"conv2\")\n",
    "conv3, conv3_weights = conv_layer(conv2,3,64,128,2,\"conv3\")\n",
    "res1, res1_weights = residual_layer(conv3,3,128,128,1,\"res1\")\n",
    "res2, res2_weights = residual_layer(res1,3,128,128,1,\"res2\")\n",
    "deconv1 = deconvolution_layer(res2,conv2_weights,[computed_batch_size,48,48,64],2,'deconv1')\n",
    "deconv2 = deconvolution_layer(deconv1,conv3_weights,[computed_batch_size,96,96,32],2,'deconv2')\n",
    "\n",
    "conv4, conv4_weights = conv_layer(deconv2,9,32,3,1,\"last_layer\",activation_function=tf.nn.tanh)\n",
    "output = output_between_zero_and_one(conv4)\n",
    "\n",
    "\n",
    "pixel_loss = get_pixel_loss(output_placeholder,output)\n",
    "style_loss = get_style_loss(output_placeholder,output)\n",
    "smooth_loss = get_smooth_loss(output)\n",
    "\n",
    "style_factor = 1.0\n",
    "pixel_factor = 1.0\n",
    "smooth_factor = 0.0001\n",
    "\n",
    "loss = pixel_factor*pixel_loss + style_factor*style_loss+smooth_factor*smooth_loss\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "def show_progress(input_image,target,generated):\n",
    "    fig = plt.figure()\n",
    "\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(input_image,cmap='gray')\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(target)\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(generated)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Results\n",
    "After explainig the image generation methods and network, it is time for running the network! It is really interesting to see the output of the network over time. I display several of them, and all of them in a sped-up version in the GIF that is generated at the end of the training epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 10000\n",
    "batch_size = 32\n",
    "\n",
    "for epoch_i in range(n_epochs):\n",
    "    in_x, in_y = imageloader.get_train_images(batch_size)\n",
    "    if NORMALISE_INPUT:\n",
    "        in_x = normalise_numpy_images(in_x,mean_sketch,stdeviation_sketch)\n",
    "    in_x = np.expand_dims(in_x,3)\n",
    "    \n",
    "    _,l = sess.run([optimizer,loss], feed_dict={input_placeholder:in_x ,output_placeholder: in_y })\n",
    "    \n",
    "    if epoch_i % 100==1:\n",
    "        colored_images = sess.run(output, feed_dict={input_placeholder:test_x,output_placeholder:test_y})\n",
    "        generated = np.clip(colored_images,0.0,1.0)\n",
    "        generated = generated[0]\n",
    "        progress_images.append(generated)\n",
    "        if epoch_i < 800 or epoch_i > 19900:\n",
    "            show_progress(source_x[0],test_y[0],generated)\n",
    "        \n",
    "print(\"building progress gif out of \" + str(len(progress_images)) + \" images\")\n",
    "gif.build_gif(progress_images, interval=0.1, dpi=72, save_gif=True, saveto='animation.gif',show_gif=False)\n",
    "ipyd.Image(url='animation.gif', height=200, width=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Testdata \n",
    "To test my algorithm I tried to convert the following data using the trained network:\n",
    "- testdata from the celebrity dataset\n",
    "- images from people I found using Google Images by typing in \"gameboy camera\"\n",
    "- faces that are in the gameboy camera (also found online)\n",
    "- pictures of my face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Testdata from the celebrity dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for index,generated_image in enumerate(colored_images):\n",
    "    show_progress(source_x[index],test_y[index],generated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This all looks pretty good to me. This was expected, as these images were taken with the same restrictions as the trainset. \n",
    "\n",
    "### Images from the internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def show_colored_pictures(test_pictures):\n",
    "    for name_picture in test_pictures:\n",
    "        path_file = os.path.join(name_picture)\n",
    "        image = plt.imread(path_file)\n",
    "        image = cv2.resize(image,(96,96))\n",
    "        image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "        in_x = np.array([image])\n",
    "        if NORMALISE_INPUT:\n",
    "            in_x = normalise_numpy_images(in_x,mean_sketch,stdeviation_sketch)\n",
    "        in_x = np.expand_dims(in_x,3)\n",
    "\n",
    "        colored_images = sess.run(output, feed_dict={input_placeholder:in_x})\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(image,cmap='gray')\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(colored_images[0])\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "test_pictures = ['test19.png','test18.png','test1.png','test2.png','test3.png']\n",
    "show_colored_pictures(test_pictures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I was impressed with how well these images turned out given that they do not follow the pattern as the train images. Even though the eyes are on a different spot, and a larger area was cropped around the face, I think the network created pretty good images.  \n",
    "\n",
    "### Images from the gameboy itself\n",
    "When trying to display an empty animation the gameboy camera has several faces it can display warning you that you have to create an animation first. I took two of these faces and tried colorizing them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_pictures = ['test4.png','test5.png']\n",
    "\n",
    "show_colored_pictures(test_pictures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Looks pretty good to me!\n",
    "\n",
    "### Images I took\n",
    "A big problem trying to create color images from my own face was getting them off the gameboy camera. Buying the camera was easy, but finding a gameboy printer was impossible. Although somebody made a cable to put the images through the link cable on your pc, this also was impossible to find. What was left was the great method of taking images of the screen. A problem with this approach is that the lighting is always a lot off. As our network is trained on images that have equal lightning this posed a bit of a problem. This was a problem that was not easy to solve, and we have to do with colored images from noisy input. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_pictures = ['test25.png','test26.png','test27.png','test28.png','test29.png','test31.png']\n",
    "show_colored_pictures(test_pictures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output last deconvolution\n",
    "To see what the network \"learned\" the activations in the last layer can be visualised. Manually cherry-picking some interesting layers gives this result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "last_layer = sess.run(deconv2, feed_dict={input_placeholder:test_x})\n",
    "inspect_layer = last_layer[0]\n",
    "\n",
    "last_layer_activations = []\n",
    "for inspect_convolution_output in range(inspect_layer.shape[2]):\n",
    "    last_layer_activations.append(inspect_layer[:,:,inspect_convolution_output])\n",
    "\n",
    "last_activation_montage = utils.montage(last_layer_activations)\n",
    "plt.imshow(last_activation_montage,cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### PCA of each layer\n",
    "As you can see the network seems to encode features such as hair, eyes, side of the face. \n",
    "The last interesting thing I wanted to show is a visualising the principal components in each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_steps = sess.run([input_placeholder,conv1,conv2,conv3,res1,res2,deconv1,deconv2,output], feed_dict={input_placeholder:test_x,output_placeholder:test_y})\n",
    "for index_layer,layer in enumerate(all_steps[1:-1]):\n",
    "    print(\"Principal components output layer \" + str(index_layer+1))\n",
    "    first_image = layer[0]\n",
    "    original_shape = first_image.shape\n",
    "    original_dimensions = original_shape[2]\n",
    "    first_image = np.reshape(first_image, (-1,original_dimensions))\n",
    "    pca = PCA(n_components=3)\n",
    "    fitted = pca.fit_transform(first_image)\n",
    "    fitted = np.reshape(fitted,(original_shape[0],original_shape[1],-1))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(fitted[:,:,0],cmap='gray')\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(fitted[:,:,1],cmap='gray')\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(fitted[:,:,2],cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The output of the principal components is both interesting and a bit obvious. The network learns to encode the skin, hair and background of the input images (just like we seen before). \n",
    "\n",
    "### Interesting observations/lessons learned\n",
    "During this project I learned a lot of lessons. The lesson about different deconvolution layers is something I already described above. Another interesting lesson is that I started with normalising the output of the neural network.This yielded nice results early in training (outputting only zeros is already a good guess), but later this network had a lot of problems. The output of a barely trained network can be seen below. Unfortunately faces that were far away from the norm (i.e. people with hair in front of their face, sunglasses, people looking sideways) became blurry. \n",
    "\n",
    "One question I asked myself was: how does this task compare to coloring sketch images? The details of the face are very blurry, but the outline of face details is still preserved. Because the areas between features are filled with 4 colours, the network has more grasp on what the resulting colour should compared to the line sketch problem. One interesting thing is that this network gives the right skincolor to people most of the time.   \n",
    "\n",
    "## Conclusion\n",
    "Create photorealistic color images from gameboy camera images is a possibility! Going from 0.05 megapixels 4-color-grayscale images to full-color faces is something convolutional neural networks can learn. \n",
    "\n",
    "If you have other ideas for styles to convert from, or other things you would like to try, let me know. I am always willing to answer your questions. If you enjoyed reading this, please leave a comment or share this post to others who might be interested. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "in_x, in_y = get_random_line_sketches(3)\n",
    "colored_images = sess.run(output, feed_dict={input_placeholder:in_x})\n",
    "#print(colored_images[0])\n",
    "print(in_y[0])\n",
    "\n",
    "plt.imshow(in_y[0])\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(colored_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "a = get_random_gameboy_camera(2)\n",
    "\n",
    "plt.imshow(a[0][0],cmap='gray')\n",
    "#         gray_image = rgb2gray(image)\n",
    "#         Xs.append(gray_image)\n",
    "#         Ys.append(image)\n",
    "#     return np.array(Xs), np.array(Ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "first_image = plaything[0]\n",
    "print(first_image.shape)\n",
    "for i in range(first_image.shape[2]):\n",
    "    plt.imshow(first_image[:,:,i],cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "first_image = plaything[0]\n",
    "print(first_image.shape)\n",
    "first_image = np.reshape(first_image, (-1,32))\n",
    "print(first_image.shape)\n",
    "pca = PCA(n_components=3)\n",
    "fitted = pca.fit_transform(first_image)\n",
    "print(fitted.shape)\n",
    "fitted = np.reshape(fitted,(96,96,-1))\n",
    "print('whoo')\n",
    "print(fitted.shape)\n",
    "plt.imshow(fitted)\n",
    "plt.show()\n",
    "plt.imshow(fitted[:,:,0],cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(fitted[:,:,1],cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(fitted[:,:,2],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(conv_weights_first_layer.shape)\n",
    "for i in range(conv_weights_first_layer.shape[3]):\n",
    "    layer_here = conv_weights_first_layer[:,:,:,i]\n",
    "    layer_here = layer_here[:,:,0]\n",
    "    print(layer_here.shape)\n",
    "    \n",
    "    #layer_here = np.reshape(layer_here,(9,9,32,1))\n",
    "    plt.imshow(layer_here,cmap='gray',interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "g = tf.Graph()\n",
    "with tf.Session(graph=g) as sess, g.device('/cpu:0'):\n",
    "    tf.import_graph_def(net['graph_def'], name='vgg')\n",
    "    names = [op.name for op in g.get_operations()]\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "[name_i for name_i in names if 'conv' in name_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from skimage.data import coffee\n",
    "\n",
    "# output layer marcel uses: 'vgg/conv2_2/conv2_2'\n",
    "# but with a relu something on it....\n",
    "# which this one does not seem te have...\n",
    "\n",
    "og = coffee()\n",
    "plt.imshow(og)\n",
    "img = vgg16.preprocess(og)\n",
    "img_4d = img[np.newaxis]\n",
    "x = g.get_tensor_by_name(names[0] + ':0')\n",
    "softmax = g.get_tensor_by_name(names[-2] + ':0')\n",
    "\n",
    "with tf.Session(graph=g) as sess, g.device('/cpu:0'):\n",
    "    res = softmax.eval(feed_dict={x: img_4d})[0]\n",
    "    print([(res[idx], net['labels'][idx])\n",
    "           for idx in res.argsort()[-5:][::-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "img_4d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#tested_targets = in_y\n",
    "#tested_predictions = colored_images\n",
    "img_4d = np.array(vgg16.preprocess(tested_predictions[0]))\n",
    "print(img_4d.shape)\n",
    "img_4d = np.array([img_4d])\n",
    "\n",
    "x = g.get_tensor_by_name(names[0] + ':0')\n",
    "softmax = g.get_tensor_by_name(names[-2] + ':0')\n",
    "\n",
    "with tf.Session(graph=g) as sess, g.device('/cpu:0'):\n",
    "    res = softmax.eval(feed_dict={x: img_4d})[0]\n",
    "    print([(res[idx], net['labels'][idx])\n",
    "           for idx in res.argsort()[-5:][::-1]])\n",
    "    \n",
    "with tf.Session(graph=g) as sess, g.device('/cpu:0'):\n",
    "    content_layer = 'vgg/conv2_2/conv2_2:0'\n",
    "    content_features = g.get_tensor_by_name(content_layer).eval(\n",
    "            session=sess,\n",
    "            feed_dict={x: img_4d,\n",
    "                'vgg/dropout_1/random_uniform:0': [[1.0] * 4096],\n",
    "                'vgg/dropout/random_uniform:0': [[1.0] * 4096]\n",
    "            })\n",
    "print(content_features.shape)\n",
    "to_show = content_features[0]\n",
    "for i in range(128):\n",
    "    plt.imshow(to_show[:,:,i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.misc import imread, imresize\n",
    "from imagenet_classes import class_names\n",
    "from vgg16 import vgg16\n",
    "\n",
    "PREPROCESSED_IMAGE_PATH = \"/home/roland/clinworkspace/imagesFollowing4\"\n",
    "PROCESSED_PICTURE_DATASET = os.listdir(PREPROCESSED_IMAGE_PATH)\n",
    "\n",
    "# sess = tf.Session()\n",
    "# imgs = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "# vgg = vgg16(imgs, 'vgg16_weights.npz', sess)\n",
    "for image_name in PROCESSED_PICTURE_DATASET:\n",
    "    image_name = os.path.join(PREPROCESSED_IMAGE_PATH,image_name)\n",
    "    img1 = imread(image_name)\n",
    "    img1 = imresize(img1, (224, 224))\n",
    "\n",
    "    w, h = img1.shape\n",
    "    ret = np.empty((w, h, 3), dtype=img1.dtype)\n",
    "    ret[:, :, 0] = img1\n",
    "    ret[:, :, 1] = img1\n",
    "    ret[:, :, 2] = img1\n",
    "    img1 = ret\n",
    "    plt.imshow(img1)\n",
    "    plt.show()\n",
    "\n",
    "    prob = sess.run(vgg.probs, feed_dict={vgg.imgs: [img1]})[0]\n",
    "    preds = (np.argsort(prob)[::-1])[0:5]\n",
    "    \n",
    "    for p in preds:\n",
    "        print(class_names[p], prob[p])\n",
    "# net = vgg16.get_vgg_model()\n",
    "\n",
    "# print([n.name for n in net['graph_def'].node])\n",
    "\n",
    "# # #print()\n",
    "# # #style_layer = 'vgg/conv2_1/conv2_2:0'\n",
    "# feature_transformed_target = tf.import_graph_def(\n",
    "#         net['graph_def'],\n",
    "#         name='vgg',\n",
    "#         input_map={'images:0': output_placeholder},return_elements=[\"conv2_2/conv2_2:1\"])\n",
    "# print(output_placeholder)\n",
    "# print(final_output)\n",
    "# feature_transformed_prediction = tf.import_graph_def(\n",
    "#         net['graph_def'],\n",
    "#         name='vgg',\n",
    "#         input_map={'images:0': final_output},return_elements=[\"conv2_2/conv2_2:1\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
